{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c488dd2",
   "metadata": {},
   "source": [
    "# Lab 4 - Information Theory in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b229d4",
   "metadata": {},
   "source": [
    "## Part 1: Entropy and Information Gain in Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14984eb",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1a529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424066b",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23e5daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598684f5",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab7832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785dcdf",
   "metadata": {},
   "source": [
    "What is your observastion about the calculated Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a496e3",
   "metadata": {},
   "source": [
    "The function uses the standard formula for entropy and assumes discrete class labels. It can be applied to both binary and multiclass problems. Higher entropy signifies more disorder, while lower entropy implies more homogeneity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6276a2",
   "metadata": {},
   "source": [
    "## Step 4: Calculate Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7c0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(df, attribute, target_name):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6171c9",
   "metadata": {},
   "source": [
    "Discuss your findings here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09501552",
   "metadata": {},
   "source": [
    " It calculates the information gain by comparing the total entropy of the dataset with the weighted average entropy of subsets created by splitting based on the attribute. Higher information gain indicates a better attribute for splitting. The function assumes non-empty sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfa819",
   "metadata": {},
   "source": [
    "## Part 2: Apply Entropy and Information Gain on a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6090d0",
   "metadata": {},
   "source": [
    "## Task 1: Implement Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59195699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "df_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df_cancer['target'] = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "622dceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in class_labels:\n",
    "        probability = len(y[y == label]) / len(y)\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3bb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(df, attribute, target_name):\n",
    "    total_entropy = calculate_entropy(df[target_name])\n",
    "    values, counts = np.unique(df[attribute], return_counts=True)\n",
    "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df.where(df[attribute] == values[i]).dropna()[target_name]) for i in range(len(values)))\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796c8c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for 'mean radius': 0.8607815854835991\n",
      "Information Gain for 'mean texture': 0.8357118798482908\n"
     ]
    }
   ],
   "source": [
    "info_gain_radius_mean = calculate_information_gain(df_cancer, 'mean radius', 'target')\n",
    "info_gain_texture_mean = calculate_information_gain(df_cancer, 'mean texture', 'target')\n",
    "\n",
    "print(\"Information Gain for 'mean radius':\", info_gain_radius_mean)\n",
    "print(\"Information Gain for 'mean texture':\", info_gain_texture_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755ede7",
   "metadata": {},
   "source": [
    "## Task 2: Discuss your findings in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c521b1f",
   "metadata": {},
   "source": [
    "Provide detailed explanation and discussion about your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f33ce",
   "metadata": {},
   "source": [
    "The 'mean radius' Information Gain (0.8608):\n",
    "Interpretation: 'mean radius' is a highly useful feature for distinguishing between malignant and benign tumors.\n",
    "Implication: In a decision tree, 'mean radius' would likely be chosen early for splitting nodes due to its high information gain.\n",
    "\n",
    "The'mean texture' Information Gain (0.8357):\n",
    "Interpretation: 'mean texture' is also valuable for classifying tumors as malignant or benign.\n",
    "Implication: It is likely to be an important feature in decision tree nodes, contributing to effective class separation.\n",
    "\n",
    "Comparison:\n",
    "Both features are highly informative, but 'mean radius' has a slightly higher information gain.\n",
    "In decision tree construction, both features are expected to play key roles in creating more homogenous subsets.\n",
    "\n",
    "In summary, both 'mean radius' and 'mean texture' are valuable for classifying breast tumors, with 'mean radius' standing out slightly in this analysis. These insights guide decision tree construction for effective classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
